[![Python 3.7.5](https://img.shields.io/badge/python-3.7.5-blue.svg)](https://www.python.org/downloads/release/python-375/)
[![black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# SIFAL : Sampling Instances for Accelerated Learning

Table of Content:
 - [Motivation](#Motivation)
   - [Context](#Context)
   - [Can we do better?](#Can-we-do-better)
   - [Actual Motivation](#Actual-Motivation)
   - [Goals](#What-can-we-do)
 - [Experiment](#Experimental-Setup)
   - [Task](#Task)
     - [Base](#Base-Optimization-Strategy)
     - [Population Based Optimization](#Population-Based-Optimization-Strategy)
     - [Percentage Based Optimization](#Percentage-Based-Optimization-Strategy)
   - [Dataset](#Dataset)
   - [Featurization](#Featurization)
   - [Model](#Model)
   - [Optimization](#Optimization)
 - [Applications](#Applications)
 - [Write-up](#Write-up)

## Motivation

### Context

Historically, the machine learning models are trained in the following fashion:

1. Create batches from the dataset.
2. Define a model.
3. Optimize the model for each batch for `n_epoch` times.

<!-- We can see a few glaring flaws in this approach.  -->

There are a couple of issues with this approach:
1. As the number of epochs reach `n_epoch`, there will be certain patterns in the data that the model will have already learned. It makes little sense to include such patterns again in the batches and/or dataset for the model to learn.
2. The loss from instances where it makes glaring mistakes will be overshadowed by accumulated marginal loss from the trivial/almost-learned instances. The algebraic correlation `He:Doctor::she:Nurse` from `word2vec` is an (in)famous example of this.

### Can we do better? 
The main question we are trying to ask here and get an answer to is:

Can we ***select instances to learn from*** based on the loss generated by each isntance?

### Actual Motivation : Karpathy Strikes Again!!

The main motivating idea behind this `SIFAL`  project was infact [a tweet by Karpathy](https://twitter.com/karpathy/status/1311884485676294151) which suggested that there wIll be something interesting if we just "sorted the dataset descending by loss". This project is the result of asking a lot of questions to this suggestion.

### What can we do?
Well, what exactly? What would be so interesting if we sorted the dataset descending by loss? 

1. Since we know the loss for each instance in the dataset, we can manually check instances with highest losses and lowest losses. This can be done at each epoch but makes sense to do it at the end of the training to assess which classes are harder to learn.
2. We can plot the distribution of labels for each decile in the loss range. Alternatively, we can plot the most frequent label for each decile in the loss range.
3. We can learn from the top `x%` of the instance population and see if the loss also reduces for the remaining `(100-x)%` of the instance population _and_ the test dataset.
4. We can learn from the instances that cover the top `x%` of the total dataset loss and learn from those and test accordingly.
5. We can learn from the instances that cover the bottom `x%` (of the total dataset loss or the total instance population) and test accordingly. Will this optimization get stuck in the crest region of the hyperspaces?
6. Can we draw/address parallels to the societal biases from this procedure?

## Experimental Setup

### Task
To test the suggestion given by Karpathy and address the flaws of traditional ML model optimization, we will try the three optimization strategies mentioned below and compare the results:

#### Base Optimization Strategy

_Traditional Method_

We will train the model as usual: train the model `n_epochs` times, on each instance from the dataset, irrespective of the prediction error on the instance. This is how the ML models are trained and will use this as the benchmark for comparing other strategies.

#### Population Based Optimization Strategy : `%tile`

_Percentile Sampling of Instances_

For every epoch, **Sort the dataset descending by loss from each instance**, take the top `x%` of this _sorted instance populaion_ and train the model on losses from these instances. 

#### Percentage Based Optimization Strategy : `%age`

_Percentage Sampling of Instances_

For every epoch, **Sort the dataset descending by loss from each instance**, take instances which contribute to the top `x%` of _total loss from the dataset_ and train the model on losses from these instances.


### Dataset

The dataset for this experiment is an NLP dataset of intent classification on _real_ customer queries where the text is the query from a customer and the intent is the label given to it by the Customer Support Agent.

There are in total `15K+` instances in the dataset with `32` intent types. The `train:test` split of the dataset is `70:30`. The sample distribution by label can be found here [#TODO](INSERT_LABEL_DISTRIBUTION_IMG).

An instance from the dataset:
```
Input: "Can you plesae calll me back"
Output: user_request_callback
```

<span style="color:red">Reproducibility Note</span>: The training dataset and the testing dataset for this experiment are fixed and the ordering of instances in the datasets are also fixed. Meaning training/testing instances will _not_ be shuffled in the entirety of this experiment. This is so that reproducibility can be maintained for different runs of this experiment.

### Featurization
There are two parts here: preprocessing and tokenization of the input text. We use a simple regex based preprocessor, and tokenize the text with unigrams and bi-grams and build a Bag-of-Words feature based on count of unigrams and bigrams from the text.


### Model

The task at hand here is of intent classification. So for that we use a simple fully connected neural network with one hidden layer.

```%python
BoW Features --> Input Layer --> Hidden Layer --> Output Layer
```

<span style="color:red">Reproducibility Note</span>: The most important thing in this experiment is the random seed or the random initialization of the neural network weights. In this experiment, we initialize the network from saved weights so that the network is always initialised the same way for different runs.

### Optimization

For optimization, we use the `3` strategies mentioned in the [Task](#Task) section above.

### Evaluation

We have a number of way to evaluate this experiment:

1. The usual suspects : accuracy, precision, recall, f1, classification report, etc.
2. The distribution of labels after training/certain epochs for each training strategy.
3. The distribution of weights after training/certain epochs for each training strategy.
4. Sparsity of weights during and/or after training for each training strategy.
5. KL Divergence of weights during and/or after training for each training strategy: from `0`, `1`, init weights and optimized weights using base strategy, does it increase or decrease? 
6. More the KL, more the learning??


## Applications
1. Transfer Learning
1. Fine tuning

## Write-up
1. Write a blog or medium articles.